## 逻辑回归
1. 损失函数推导，损失函数求解
2. 几种梯度下降法的特点， LR与感知机、SVM的区别和联系


## 感知机和BP的联系， 为什么要用对偶形式求解，联系感知机解释线性可分



## SVM原理，对偶形式 核函数的作用和选取方式， 核函数的调参、 SMO算法的原理


## ID3、c4.5 ,CART树的特点

## GBDT 、 FR 、 boosting 、 bagging 、 stacking , GBDT和xgboost区别和联系





# 统计学习方法


## 统计学习方法概率
1. 统计学习三要素：模型、策略和算法
2. 模型评估和模型选择
    - 训练误差和测试误差
    - 过拟合问题
3. 正则化和交叉验证
4. 泛化能力
5. 生成模型和判别模型
6. 分类问题、标注问题和回归问题


## 感知机

1. 线性分类，主要是学习其对偶形式

[Perceptron](Perceptron.md)


## KNN

[KNN](KNearestNeighbors.md)


## 决策树

步骤：

1. 特征选取（信息增益和信息增益比）
2. 树的构建
3、树的剪枝（防止过拟合）

- ID3、 ID4.5 、 CART算法

1. 熵的定义

H(p) = - sum(pi * log(pi))
0<= H(p)<=logn

2. 信息增益

得知特征X的信息而使得类Y的信息的不确定性减少的程度。

3. 剪枝
损失函数的最小化原则就是正则化的极大似然估计进行模型选择。

[Decision Tree](DecisionTree.md)


## logistic Regression

1. 梯度下降法




predict = sigmoid(w*x)
error = lable - predict
dw += learning * error * X.T

[logistic Regression](machineLearning\LogisticRegression.md)

## 神经网路
1. 神经元模型
2. 感知机和多层网络
3. BP算法推导
    - 标准BP
    - 累计BP，一个epoch才更新，随机梯度下降法
4. 其他类型神经网络

todo :代码实现

## SVM

1. 为什么要使用对偶问题求解

- 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
- 方便核函数的引入
- 改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。

所以，如果是做线性分类，且样本维度低于样本数量的话，在原问题下求解就好了，Liblinear之类的线性SVM默认都是这样做的；但如果你是做非线性分类，那就会涉及到升维（比如使用高斯核做核函数，其实是将样本升到无穷维），升维后的样本维度往往会远大于样本数量，此时显然在对偶问题下求解会更好。

2. [SMO算法](SVM.md)



## 提升方法

提升(boosting)方法是一种常用的统计学习方法，它改变训练样本的权重，学习多个分类器，并将这些分类器线性组合，提高分类的性能。
1. 每一轮如何改变训练数据的权值或者概率分布
2. 将弱分类器组合成一个强分类器

## Adaboost算法

[Adaboost](AdaBoost.md)


## 马尔科夫链

A是个二维矩阵，`a[i] for a in A`则表示取A举证的i列。

```
                    alphas[i][t] = np.dot(
                        [alpha[t - 1] for alpha in alphas],
                        [a[i] for a in A]) * B[i][indexOfO]  # 对应P176（10.16）
```



## 概率图模型


## 降维与度量学习
1. K近邻学习

KNN:监督的学习方法，泛化错误率不超过贝叶斯最优分类器的错误率的两倍。

2. 低维嵌入
    原始空间中的样本之间的距离在低维空间中得以保持，如MDS算法

3. 主成成分分析（PCA）

4. 和化线性降维

5. 流形学习

6. 度量学习(metric learning)：直接学习出一个合适的距离度量，而不用通过降维的方式
    - W是半正定的

## 8/28 刷剧。。。

## 特征选择与稀疏学习
1. 特征选择
    - 过滤式选择
    - 包裹式选择
2. 稀疏学习

3. 压缩感知


## 半监督学习

1. 生产式方法

直接基于生成式模型的方法，可基于EM算法进行极大似然估计求解，区别主要在于生成式模型的假设

2. 半监督SVM

3. 图半监督学习

4. 基于分歧的方法： 使用多学习器，相互学习，共同进步

5. 半监督分类
    - 必连和勿连约束： 约束K均值算法
    - 少量的标记样本： 约束种子k均值算法


## 概率图模型
1. 隐马尔科夫模型
    - 3个基本问题
2. 马尔科夫随机场（MRF）
    - 马尔科夫网
    - 全局马尔科夫性
    - 局部马尔可夫性
    - 成对马尔可夫性
3. 条件随机场
    - 判别式无向图模型，对条件分布进行建模
4. 学习与推断
    - 变量消去
    - 信念传播
5. 近似推断
    - MCMC采样：马尔科夫蒙特卡洛
    - 变分推断
6. 话题模型
    - 隐狄利克雷分配模型（LDA）


## 规则学习
1. 命题规则和一阶规则

2. 序贯覆盖： 逐条归纳

3. 剪枝优化：规则生成是一个贪心搜索过程
    - CN2 : 优于直接基于训练样例集后验概率分布进行预测
    - REP： 后剪枝
    - RIPPER:剪枝策略和后处理手段结合起来对规则集进行优化，性能超过决策树

4. 一阶规则学习
    - FOIL算法： 自顶向下规则归纳

5. 归纳逻辑程序设计： ILP
    - 最小一般泛化
    - 逆归结：归纳演绎， 归结原理和逆归结原理

## 强化学习
1. K-摇臂赌博机
    - 探索与利用
    - e-贪心
    - SotfMax
2. 有模型学习
    - 策略评估（侧率π指的什么）
        - 状态值函数
        - 状态-动作值函数
    - 策略改进
    - 策略迭代与值迭代
3. 免模型学习
    - 蒙特卡罗强化学习
    - 时序差分学习
        - Sarsa:on-policy
        - Sarsa修改为异策略算法(off-policy)，得到Q-Learning
4. 值函数近似
5. 模仿学习
    - 直接模仿学习
    - 逆强化学习

## TODO：对每章的算法使用MINST来验证

## Reference
[lihang-code](https://github.com/fengdu78/lihang-code)